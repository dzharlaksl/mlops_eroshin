# mlops_eroshin
 MLops URFU for dz

# Конвейер автоматизированного машинного обучения для линейной регрессии

# Описание

Автоматизированный конвейер для создания наборов данных, предварительной обработки данных, обучения модели линейной регрессии и тестирования ее производительности. Сценарии разделены на этапы, и для бесперебойного выполнения всего конвейера от начала до конца предусмотрен сценарий bash.

# Структура проекта

- datacreation.py: Генерирует обучающие и тестовые наборы данных.
- model_preprocessing.py: Предварительно обрабатывает наборы данных (возможности масштабирования).
- model_preparation.py: Обучает модель линейной регрессии.
- model_testing.py: Тестирует обученную модель и вычисляет показатели производительности.
- pipeline.sh: Запускает скрипт Bash для выполнения всего конвейера.

# Установка

Клонируем репозиторий:
git clone https://github.com/dzharlaksl/mlops_eroshin.git

# Использование

Создайте и активируйте виртуальную среду.
Убедитесь, что у вас установлен Python и необходимые библиотеки:

pip install numpy pandas scikit-learn joblib

Сделайте скрипт pipeline.sh исполняемым:

chmod +x pipeline.sh

Запустите весь конвейер:

./pipeline.sh

# Подробные шаги

# Шаг 1: Создание данных

`datacreation.py`: Этот скрипт генерирует обучающие и тестовые наборы данных. Обучающие данные содержат шум, в то время как обычные тестовые данные - нет. Также создается дополнительный тестовый набор с шумом.

datacreation.py создаем и импортируем библиотеки
Создаем каталоги
Создание и сохранение наборов данных

# Шаг 2: Предварительная обработка данных

`model_preprocessing.py`: Этот скрипт обрабатывает обучающие и тестовые наборы данных путем масштабирования объектов.

model_preprocessing.py создаем и импортируем библиотеки
Загружаем наборы данных
Масштабирование данных (стандартное)
Сохранить обработанные данные

# Шаг 3: Обучение модели

`model_preparation.py`: Этот скрипт обучает модель линейной регрессии, используя обработанные обучающие данные.

model_preparation.py создаем и импортируем библиотеки
Загружаем обработанные обучающие данные
Обучаем модель линейной регрессии
Сохраняем обученную модель

# Шаг 4: Тестирование модели

`model_testing.py`: Этот скрипт тестирует обученную модель линейной регрессии, используя обработанные данные тестирования, и вычисляет среднеквадратичную ошибку.

model_testing.py создаем и импортируем библиотеки
Загружаем обработанные данные тестирования и сохраненную модель
Тестируем модель и выводим среднеквадратичную ошибку

# Шаг 5: Запустите конвейер

`pipeline.sh `: Этот bash-скрипт выполняет все шаги последовательно.

# Автор
Игорь Ерошин



Next labs!!!!!

## Module 2
<details>

* Вам нужно разработать собственный конвейер автоматизации для проекта машинного обучения. Для этого вам понадобится виртуальная машина с установленным Jenkins, python и необходимыми библиотеками. В ходе выполнения практического задания вам необходимо автоматизировать сбор данных, подготовку датасета, обучение модели и работу модели.
* Разработанный конвеер требуется выгрузить в файл. Так же все скрипты (этапы конвеера требуется сохранить)
* Все файлы необходимо разместить в подкаталоге lab2 корневого каталога
Этапы задания

1. Развернуть сервер с Jenkins, установить необходимое программное обеспечение для работы над созданием модели машинного обучения.
2. Выбрать способ получения данных (скачать из github, из Интернета, wget, SQL запрос, …).
3. Провести обработку данных, выделить важные признаки, сформировать датасеты для тренировки и тестирования модели, сохранить.
4. Создать и обучить на тренировочном датасете модель машинного обучения, сохранить в pickle или аналогичном формате.
5. Загрузить сохраненную модель на предыдущем этапе и проанализировать ее качество на тестовых данных. 
6. Реализовать задания и конвеер. Связать конвеер с системой контроля версий. Сохранить конвеер.

</details>

## Module 3
<details>

В практическом задание по модулю вам необходимо применить полученные знания по работе с docker (и docker-compose). Вам необходимо использовать полученные ранее знания по созданию микросервисов. В этом задании необходимо развернуть микросервис в контейнере докер. Например, это может быть модель машинного обучения, принимающая запрос по API и возвращающая ответ. Вариантом может быть реализация приложения на основе streamlit (https://github.com/korelin/streamlit_demo_app).
Результаты работы над этой работой стоит поместить в подкаталог lab3 вашего корневого каталога репозитория.
Что необходимо выполнить:
* Подготовить python код для модели и микросервиса
* Создать Docker file
* Создать docker образ
* Запустить docker контейнер и проверить его работу

Дополнительными плюсами будут:
1. Использование docker-compose
2. Автоматизация сборки образа привязка имени тэга к версии сборки (sha-коммита, имя ветки)
3. Деплой (загрузка) образа в хранилище артефактов например dockerhub

</details>

## Module 4
<details>

В практическом задании данного модуля вам необходимо продемонстрировать навыки практического использования утилиты dvc для работы с данными. В результате выполнения этих заданий вы выполните все основные операции с dvc и закрепите полученные теоретические знания практическими действиями.

Этапы задания:

1. Создайте папку lab4 в корне проекта.
2. Установите git и dvc. Настройте папку проекта для работы с git и dvc.
3. Настройте удаленное хранилище файлов, например на Google Disk или S3.
4. Создайте датасет, например, о пассажирах “Титаника” catboost.titanic().
5. Модифицируйте датасет, в котором содержится информация о классе (“Pclass”),  поле (“Sex”) и возрасте (“Age”) пассажира. Сделайте коммит в git и push в dvc.
6. Создайте новую версию датасета, в котором пропущенные (nan) значения в поле “Age” будут заполнены средним значением. Сделайте коммит в git и push в dvc.
7. Создайте новый признак с использованием one-hot-encoding для строкового признака “Пол” (“Sex”). Сделайте коммит в git и push в dvc.
8. Выполните переключение между всеми созданными версиями датасета.

При правильном выполнении задания и вас появится git репозиторий с опубликованной метаинформацией и папка на Google Disk, в которой хранятся различные версии датасетов.
Вам необходимо подготовить отчет в тех функциональностях которые вы настроили. Дополнительно можно настроить DAG, запуск и версионирование экспериментов, например, с использованием Hydra.

В постановке задачи используется датасет из конкурса “Titanic Disaster”, однако вы можете использовать свои наборы данных, в этом случае в п.п.4-8 необходимо использовать информацию и признаки из вашего датасета.

</details>

## Module 6
<details>

### Тестирование качества работы моделей машинного обучения
Цель задания: применить средства автоматизации тестирования python для автоматического тестирования качества работы модели машинного обучения на различных датасетах. Результаты размещаются в каталоге lab5.

Этапы задания:  
1. Создать три датасета с «качественными» данными, на которых можно обучить простую модель линейной регрессии, например

![image clean data](./lab5/clean_data.png)

2. На одном из этих датасетов обучить модель линейной регрессии
3. Создать датасет с шумом в данных, например

![image clean data](./lab5/noised_data.png)

4. Провести тестирование работы модели на разных датасетах с использованием pytest, анализируя качество предсказания, обнаружить проблему на датасете с шумами.

Критерии: данное задание необходимо полностью выполнить в виде jupyter ноутбука и предоставить его на проверку.

Подсказка: вы можете записать содержимое ячейки jupyter ноутбука в отдельный файл с помощью команды

```%%writefile”имя файла”```

А также можете выполнить любую linux команду прямо из ячейки jupyter ноутбука, с помощью синтаксиса

```! “имя команды”```

</details>